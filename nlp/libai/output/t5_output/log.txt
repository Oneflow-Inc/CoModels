[10/12 03:29:57] libai INFO: Rank of current process: 0. World size: 1
[10/12 03:29:57] libai INFO: Command line arguments: Namespace(config_file='configs/t5_large_pretrain.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[10/12 03:29:57] libai INFO: Contents of args.config_file=configs/t5_large_pretrain.py:
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mevaluation[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mPPLEvaluator[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mt5[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mt5_dataset[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mgraph[39m

[38;5;15mvocab_file[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/data/home/lifei/data/bert_data/bert-base-chinese-vocab.txt[39m[38;5;186m"[39m
[38;5;15mdata_prefix[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/data/home/lifei/data/bert_data/loss_compara_content_sentence[39m[38;5;186m"[39m

[38;5;15mtokenization[39m[38;5;204m.[39m[38;5;15mtokenizer[39m[38;5;204m.[39m[38;5;15mvocab_file[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mvocab_file[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;204m.[39m[38;5;15mdata_prefix[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mdata_prefix[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;204m.[39m[38;5;15mindexed_dataset[39m[38;5;204m.[39m[38;5;15mdata_prefix[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mdata_prefix[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;204m.[39m[38;5;15mdataset[39m[38;5;204m.[39m[38;5;15mdata_prefix[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mdata_prefix[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;204m.[39m[38;5;15mdataset[39m[38;5;204m.[39m[38;5;15mindexed_dataset[39m[38;5;204m.[39m[38;5;15mdata_prefix[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mdata_prefix[39m

[38;5;245m# T5-large model config[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mcfg[39m[38;5;204m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mcfg[39m[38;5;204m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mcfg[39m[38;5;204m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m6[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15minput_placement_device[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcpu[39m[38;5;186m"[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdist[39m[38;5;204m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m [39m[38;5;204m*[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mcfg[39m[38;5;204m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mamp[39m[38;5;204m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mevaluation[39m[38;5;204m.[39m[38;5;15mevaluator[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mPPLEvaluator[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./output/t5_output[39m[38;5;186m"[39m

[10/12 03:29:57] libai INFO: Full config saved to ./output/t5_output/config.yaml
[10/12 03:29:57] lb.engine.default INFO: > compiling dataset index builder ...
[10/12 03:29:57] lb.engine.default INFO: >>> done with dataset index builder. Compilation time: 0.077 seconds
[10/12 03:29:57] lb.engine.default INFO: >>> done with compiling. Compilation time: 0.078 seconds
[10/12 03:29:57] lb.engine.default INFO: Prepare training, validating, testing set
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: reading document index...
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: Finished creating indexed dataset in 0.101269 seconds
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[10/12 03:29:57] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:  > loading indexed mapping from /data/home/lifei/data/bert_data/loss_compara_content_sentence_t5_indexmap_160000mns_510msl_0.10ssp_1234s.npy
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:     loaded indexed file in 0.003 seconds
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:     total number of samples: 227689
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding [BOS] to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding [EOS] to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_0> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_1> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_2> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_3> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_4> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_5> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_6> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_7> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_8> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_9> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_10> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_11> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_12> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_13> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_14> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_15> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_16> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_17> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_18> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_19> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_20> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_21> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_22> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_23> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_24> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_25> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_26> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_27> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_28> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_29> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_30> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_31> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_32> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_33> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_34> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_35> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_36> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_37> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_38> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_39> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_40> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_41> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_42> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_43> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_44> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_45> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_46> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_47> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_48> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_49> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_50> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_51> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_52> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_53> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_54> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_55> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_56> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_57> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_58> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_59> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_60> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_61> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_62> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_63> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_64> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_65> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_66> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_67> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_68> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_69> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_70> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_71> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_72> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_73> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_74> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_75> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_76> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_77> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_78> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_79> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_80> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_81> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_82> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_83> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_84> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_85> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_86> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_87> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_88> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_89> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_90> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_91> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_92> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_93> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_94> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_95> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_96> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_97> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_98> to the vocabulary
[10/12 03:29:57] lb.tokenizer.tokenization_base INFO: Adding <extra_id_99> to the vocabulary
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:  > loading indexed mapping from /data/home/lifei/data/bert_data/loss_compara_content_sentence_t5_indexmap_9600000mns_510msl_0.10ssp_1234s.npy
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:     total number of samples: 9601788
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:  > loading indexed mapping from /data/home/lifei/data/bert_data/loss_compara_content_sentence_t5_indexmap_3200000mns_510msl_0.10ssp_1234s.npy
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[10/12 03:29:57] lb.data.data_utils.dataset_utils INFO:     total number of samples: 3200102
[10/12 03:29:59] lb.engine.default INFO: Prepare testing set
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: reading document index...
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: Finished creating indexed dataset in 0.111415 seconds
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[10/12 03:29:59] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[10/12 03:29:59] lb.data.data_utils.dataset_utils INFO:  > loading indexed mapping from /data/home/lifei/data/bert_data/loss_compara_content_sentence_t5_indexmap_10mns_510msl_0.10ssp_1234s.npy
[10/12 03:29:59] lb.data.data_utils.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[10/12 03:29:59] lb.data.data_utils.dataset_utils INFO:     total number of samples: 119906
[10/12 03:29:59] lb.engine.default INFO: Auto-scaling the config to train.train_iter=10000, train.warmup_iter=0
[10/12 03:29:59] libai INFO: > Start building model...
[10/12 03:30:01] lb.engine.default INFO: Model:
T5ForPreTraining(
  (t5_model): T5Model(
    (embedding): T5Embedding(
      (word_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): ExtendedMask()
    (encoder): Sequential(
      (layers): ModuleList(
        (0): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (1): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (2): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (3): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (4): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (5): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
      )
      (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): Sequential(
      (layers): ModuleList(
        (0): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (cross_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=True
            (dropout): Dropout(p=0.1, inplace=False)
            (query): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
            (key_value): Linear1D(in_features=384, out_features=768, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_cross_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (1): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (cross_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=True
            (dropout): Dropout(p=0.1, inplace=False)
            (query): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
            (key_value): Linear1D(in_features=384, out_features=768, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_cross_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (2): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (cross_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=True
            (dropout): Dropout(p=0.1, inplace=False)
            (query): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
            (key_value): Linear1D(in_features=384, out_features=768, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_cross_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (3): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (cross_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=True
            (dropout): Dropout(p=0.1, inplace=False)
            (query): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
            (key_value): Linear1D(in_features=384, out_features=768, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_cross_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (4): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (cross_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=True
            (dropout): Dropout(p=0.1, inplace=False)
            (query): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
            (key_value): Linear1D(in_features=384, out_features=768, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_cross_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
        (5): TransformerLayer(
          (drop_path): Identity()
          (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (self_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=False
            (dropout): Dropout(p=0.1, inplace=False)
            (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (cross_attention): MultiheadAttention(
            hidden_size=384, num_heads=12, is_cross_attention=True
            (dropout): Dropout(p=0.1, inplace=False)
            (query): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
            (key_value): Linear1D(in_features=384, out_features=768, bias=True, parallel=col)
            (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
          )
          (post_cross_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
            (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
            (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
          )
        )
      )
      (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (lm_head): LMLogits()
  )
  (loss_func): T5Loss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[10/12 03:30:01] libai INFO: >>> done with building model. Building time: 1.636 seconds
[10/12 03:30:01] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return CosineLR
[10/12 03:30:01] lb.engine.trainer INFO: Starting training from iteration 0
[10/12 03:30:01] lb.models.utils.graph_base INFO: Start compling the train graph which may take some time. Please wait for a moment ...
[10/12 03:30:10] lb.utils.events INFO:  eta: 0:07:43  iteration: 19/10000  consumed_samples: 320  total_loss: 9.204  time: 0.0467 s/iter  data_time: 0.0025 s/iter total_throughput: 342.34 samples/s lr: 1.00e-04  
[10/12 03:30:11] lb.utils.events INFO:  eta: 0:07:41  iteration: 39/10000  consumed_samples: 640  total_loss: 8.779  time: 0.0460 s/iter  data_time: 0.0018 s/iter total_throughput: 347.64 samples/s lr: 1.00e-04  
